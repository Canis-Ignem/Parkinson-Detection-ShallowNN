{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkinson detection with a shallow neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will cover the implementation for solving the parkinson detection problem from spiral drawings with a neural network. Note that this is only one of the three propoused solutions, so in order to get a better understanding of the problem check the other two as well as the documentation abailable that goes more in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jon/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import featureExtraction as fe\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "When tackling an unbalanced data set it's important to do a proper feature selection process. First it's important to understand that the data is a time series and we will have to manage this appropriately. Since we are working with drawings and the main part parkinson will affect this drawings is in the inconsistency of the lines, I decided to measure the variance and the absolute difference from the highest and lowest values in a subset of data.\n",
    "\n",
    "Let's take a look  at the values our dataset has to offer as seen in the readme.txt file:\n",
    "\n",
    "----------------\n",
    "X ; Y; Z; Pressure; GripAngle; Timestamp; Test ID\n",
    "\n",
    "\n",
    "Test ID: \n",
    "0: Static Spiral Test ( Draw on the given spiral pattern)\n",
    "1: Dynamic Spiral Test ( Spiral pattern will blink in a certain time, so subjects need to continue on their draw)\n",
    "2: Circular Motion Test (Subjectd draw circles around the red point)\n",
    "\n",
    "----------------\n",
    "\n",
    "Let's start by deffining a function that loads the files from the folders where it is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the folders\n",
    "As can be seen using glob and and pandas we can make a list that stores the information of all the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_path = \"Data/control\"\n",
    "parkinson_path = \"Data/parkinson\"\n",
    "\n",
    "def getFiles(path):\n",
    "    all_files = glob.glob(path + \"/*.txt\")\n",
    "    data = []\n",
    "\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file,sep=\";\")\n",
    "        data.append(df)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data by test_id\n",
    "The next step is for each file to separate the data by test, so we don't mix up our data. We will iterate line from lines making a list until we find that the test ID is different if we to we save the data we got up until this moment and start a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByTest(df):\n",
    "    data = []\n",
    "    test = df.values[0][6]\n",
    "    split = []\n",
    "    cont = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.values[i][6] == test:\n",
    "            split.append(df.values[i])\n",
    "        else:\n",
    "            aux = pd.DataFrame(split)\n",
    "            data.append(aux)\n",
    "            split = []\n",
    "            test = df.values[i][6]\n",
    "            split.append(df.values[i])\n",
    "    data.append(split)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the desired features\n",
    "The next step is to extract the features we want from our data set, since we are only interested in the variance and difference of the first 5 columns we will extract it using numpy we will also add to which test the data pertains to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(df):\n",
    "    features = []\n",
    "    \n",
    "    features.append(np.var( df.values[:,0]))\n",
    "    features.append(np.var( df.values[:,1]))\n",
    "    features.append(np.var( df.values[:,2]))\n",
    "    features.append(np.var( df.values[:,3]))\n",
    "    features.append(np.var( df.values[:,4]))\n",
    "    features.append(np.argmax( df.values[:,0]) - np.argmin( df.values[:,0]))\n",
    "    features.append(np.argmax( df.values[:,1]) - np.argmin( df.values[:,1]))\n",
    "    features.append(np.argmax( df.values[:,2]) - np.argmin( df.values[:,2]))\n",
    "    features.append(np.argmax( df.values[:,3]) - np.argmin( df.values[:,3]))\n",
    "    features.append(np.argmax( df.values[:,4]) - np.argmin( df.values[:,4]))\n",
    "    features.append(df.values[0][6])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we will put it all together, and in order to maximize our learning potential we will split each test data in different subsets so we can get more information. One other thing we will do, is make different sizes of subset from the control data and the parkinson data. It's important not to skew it too much, keep it below 2:1 ratio in the number of splits. The less we alter the data the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getControlFeatures():\n",
    "    control_data = []\n",
    "    control = getFiles(control_path)\n",
    "    for i in control:\n",
    "        new_df = splitByTest(i)\n",
    "        for k in range(new_df.shape[0]):\n",
    "            last_df = np.array_split(i,700)\n",
    "            for j in last_df:\n",
    "                control_data.append(extractFeatures(j))\n",
    "    return pd.DataFrame(control_data)\n",
    "\n",
    "def getParkinsonFeatures():\n",
    "    parkinson_data = []\n",
    "    parkinson = getFiles(parkinson_path)\n",
    "    for i in parkinson:       \n",
    "        new_df = splitByTest(i)\n",
    "        for k in range(new_df.shape[0]):\n",
    "            last_df = np.array_split(i,400)\n",
    "            for j in last_df:\n",
    "                parkinson_data.append(extractFeatures(j))\n",
    "    return pd.DataFrame(parkinson_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the classes\n",
    "\n",
    "Last but not least we will get the classes, this is easy we just make an array of 0s and 1s based on how many control and parkinson clases we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClasses(control, parkinson):\n",
    "    classes = []\n",
    "    for i in range(control.shape[0]):\n",
    "        classes.append(0)\n",
    "    for i in range(parkinson.shape[0]):\n",
    "        classes.append(1)\n",
    "    res = np.array(classes)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We have set the functions to get the features in place, now is time to create our multy layer perceptron. We will set it up so it can recive as many hidden layers as we want. This will also allow you to do you own testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(data, classes, test_data, test_classes,input_size, layer_sizes):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #creating The place holders for our input and output\n",
    "    x = tf.placeholder(tf.float32, [None, input_size])\n",
    "    y = tf.placeholder(tf.float32, [None, 2])\n",
    "    #the lists of weights and biases will start empty but the layers will begin with x\n",
    "    weights = []\n",
    "    biases = []\n",
    "    layers = [x]\n",
    "    \n",
    "    #set a variable to stor the size of the last layer\n",
    "    last_layer = input_size\n",
    "    \n",
    "    #Iterate through the layers so they all feed into eachother\n",
    "    for layer, neurons in enumerate(layer_sizes):  \n",
    "        weights = weights + [tf.Variable(tf.zeros([last_layer, neurons]),name=\"layer\" + str(len(layers)) + \"_weights\")]\n",
    "                       \n",
    "        biases = biases + [tf.Variable(tf.zeros([neurons]),name=\"layer\" + str(len(layers)) + \"_biases\")]\n",
    "                       \n",
    "        layers += [tf.sigmoid(tf.add(tf.matmul(layers[len(layers)-1],weights[len(weights)-1]),biases[len(biases)-1]))]\n",
    "        last_layer = neurons\n",
    "    \n",
    "    #Add the the weights biases and last sigmoid fucntion to get the prediction\n",
    "    weights = weights + [tf.Variable(tf.zeros([last_layer, 2]),name=\"last_layer_weights\")]\n",
    "    biases = biases +[tf.Variable(tf.zeros([2]),name=\"last_layer_biases\")]                                                 \n",
    "    prediction = tf.add(tf.matmul(layers[len(layers)-1],weights[len(weights)-1]),biases[len(biases)-1])\n",
    "    \n",
    "    #Both of this loss functions work properly although the sigmoid one gives slightly better results\n",
    "    #loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=prediction)\n",
    "    \n",
    "    #the predicted class will be the one with the highest value\n",
    "    class_pred = tf.argmax(prediction, axis=1)\n",
    "    \n",
    "    #set upt the optimizer\n",
    "    lr = 0.0005\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    #Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    training_epochs = 1000 *14\n",
    "    train_n_samples = data.shape[0]\n",
    "    display_step = 2000\n",
    "\n",
    "    mini_batch_size = 250\n",
    "    n_batch = train_n_samples // mini_batch_size + (train_n_samples % mini_batch_size != 0)\n",
    "    \n",
    "    #Start training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(training_epochs):\n",
    "            i_batch = (epoch % n_batch)*mini_batch_size\n",
    "            batch = data[i_batch:i_batch+mini_batch_size, :], classes[i_batch:i_batch+mini_batch_size, :]\n",
    "            sess.run(optimizer, feed_dict={x: batch[0], y: batch[1]})\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                cost = sess.run(loss, feed_dict={x: batch[0], y: batch[1]})\n",
    "                acc = np.sum(np.argmax(test_classes, axis=1) == sess.run(class_pred, feed_dict={x: test_data}))/test_classes.shape[0]\n",
    "                print(\"Epoch:\", str(epoch+1), \"Error:\", np.mean(cost), \"Accuracy:\", acc)\n",
    "        parameters = sess.run(weights+biases)\n",
    "        test_predictions = sess.run(class_pred, feed_dict={x: test_data})\n",
    "    return parameters, test_predictions, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data\n",
    "\n",
    "Now let's see how we set up our data in order the feed it to the MLP. First lets load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = fe.getControlFeatures()\n",
    "park = fe.getParkinsonFeatures()\n",
    "target = fe.getClasses(control,park)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to put it together in order to get our train and test split. We will use numpy to concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate((control,park),axis= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get our split in two ways, one is using the StratifiedKFold function, and the other the train_test_split function. I will put the code to both below but we will only use the train_test_split since result were significantly better. It is important to use the random state sin when concatenating the data we are following a very strict order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "skf = StratifiedKFold(n_splits=50,shuffle=True)\n",
    "\n",
    "for train_index, test_index in skf.split(all_data, target):\n",
    "    train_data, test_data = all_data[train_index], all_data[test_index]\n",
    "    train_target, test_target = target[train_index], target[test_index]\n",
    "'''\n",
    "train_data, test_data, train_target, test_target = train_test_split(all_data,target,\n",
    "                                                                    test_size= 0.3, random_state=30 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since our last layer has 2 nodes in it we have to modify our target information to also have 2 columns. The best way of doing this is using the one hot techniche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = np.unique(train_target).shape[0]\n",
    "train_target = np.eye(n_classes)[train_target]\n",
    "test_target = np.eye(n_classes)[test_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final touches before feeding the data to the MLP. We will get the input size by checking how many columns our training data has, and we will also define the amount and the size of the hidden layers, for this particular case, one hidden layer of size seven works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "input_size = np.shape(train_data)[1]\n",
    "print(input_size)\n",
    "layers = np.array([7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the MLP\n",
    "Now we can feed all this to our MLP and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 Error: 0.5426574 Accuracy: 0.7096481682988756\n",
      "Epoch: 4000 Error: 0.4662113 Accuracy: 0.7214726151614074\n",
      "Epoch: 6000 Error: 0.43344012 Accuracy: 0.7977511788175553\n",
      "Epoch: 8000 Error: 0.36063316 Accuracy: 0.8176278563656147\n",
      "Epoch: 10000 Error: 0.3869553 Accuracy: 0.8121508886470802\n",
      "Epoch: 12000 Error: 0.38533595 Accuracy: 0.830177729416032\n",
      "Epoch: 14000 Error: 0.33761773 Accuracy: 0.8355458832063838\n",
      "[[ 5545  2230]\n",
      " [ 2304 17491]]\n"
     ]
    }
   ],
   "source": [
    "params, test_preds, _  = MLP(train_data, train_target, test_data, test_target,input_size, layers)\n",
    "cont1 =np.count_nonzero(test_preds)\n",
    "cont0 = len(test_preds) - cont1\n",
    "print(confusion_matrix(test_preds, np.argmax(test_target, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above with barely any training time we can solve this problem with pretty good results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
