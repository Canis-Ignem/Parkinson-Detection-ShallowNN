{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkinson detection with a shallow neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will cover the implementation for solving the parkinson detection problem from spiral drawings with a neural network. Note that this notebook contains twoo of the three propoused solutions, so in order to get a better understanding of the problem check the other two as well as the documentation abailable that goes more in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "#import featureExtraction as fe\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "When tackling an unbalanced data set it's important to do a proper feature selection process. First it's important to understand that the data is a time series and we will have to manage this appropriately. Since we are working with drawings and the main part parkinson will affect this drawings is in the inconsistency of the lines, I decided to measure the variance and the absolute difference from the highest and lowest values in a subset of data.\n",
    "\n",
    "Let's take a look  at the values our dataset has to offer as seen in the readme.txt file:\n",
    "\n",
    "----------------\n",
    "X ; Y; Z; Pressure; GripAngle; Timestamp; Test ID\n",
    "\n",
    "\n",
    "Test ID: \n",
    "0: Static Spiral Test ( Draw on the given spiral pattern)\n",
    "1: Dynamic Spiral Test ( Spiral pattern will blink in a certain time, so subjects need to continue on their draw)\n",
    "2: Circular Motion Test (Subjectd draw circles around the red point)\n",
    "\n",
    "----------------\n",
    "\n",
    "Let's start by deffining a function that loads the files from the folders where it is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the folders\n",
    "As can be seen using glob and and pandas we can make a list that stores the information of all the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_path = \"Data/control\"\n",
    "parkinson_path = \"Data/parkinson\"\n",
    "\n",
    "def getFiles(path):\n",
    "    all_files = glob.glob(path + \"/*.txt\")\n",
    "    data = []\n",
    "\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file,sep=\";\")\n",
    "        data.append(df)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data by test_id\n",
    "The next step is for each file to separate the data by test, so we don't mix up our data. We will iterate line from lines making a list until we find that the test ID is different if we to we save the data we got up until this moment and start a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByTest(df):\n",
    "    data = []\n",
    "    test = df.values[0][6]\n",
    "    split = []\n",
    "    cont = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.values[i][6] == test:\n",
    "            split.append(df.values[i])\n",
    "        else:\n",
    "            aux = pd.DataFrame(split)\n",
    "            data.append(aux)\n",
    "            split = []\n",
    "            test = df.values[i][6]\n",
    "            split.append(df.values[i])\n",
    "    data.append(split)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the desired features\n",
    "The next step is to extract the features we want from our data set, since we are only interested in the variance and difference of the first 5 columns we will extract it using numpy we will also add to which test the data pertains to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(df):\n",
    "    features = []\n",
    "    \n",
    "    features.append(np.var( df.values[:,0]))\n",
    "    features.append(np.var( df.values[:,1]))\n",
    "    features.append(np.var( df.values[:,2]))\n",
    "    features.append(np.var( df.values[:,3]))\n",
    "    features.append(np.var( df.values[:,4]))\n",
    "    features.append(np.argmax( df.values[:,0]) - np.argmin( df.values[:,0]))\n",
    "    features.append(np.argmax( df.values[:,1]) - np.argmin( df.values[:,1]))\n",
    "    features.append(np.argmax( df.values[:,2]) - np.argmin( df.values[:,2]))\n",
    "    features.append(np.argmax( df.values[:,3]) - np.argmin( df.values[:,3]))\n",
    "    features.append(np.argmax( df.values[:,4]) - np.argmin( df.values[:,4]))\n",
    "    features.append(df.values[0][6])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we will put it all together, and in order to maximize our learning potential we will split each test data in different subsets so we can get more information. One other thing we will do, is make different sizes of subset from the control data and the parkinson data. It's important not to skew it too much, keep it below 2:1 ratio in the number of splits. The less we alter the data the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getControlFeatures():\n",
    "    control_data = []\n",
    "    control = getFiles(control_path)\n",
    "    for i in control:\n",
    "        new_df = splitByTest(i)\n",
    "        for k in range(new_df.shape[0]):\n",
    "            last_df = np.array_split(i,700)\n",
    "            for j in last_df:\n",
    "                control_data.append(extractFeatures(j))\n",
    "    return pd.DataFrame(control_data)\n",
    "\n",
    "def getParkinsonFeatures():\n",
    "    parkinson_data = []\n",
    "    parkinson = getFiles(parkinson_path)\n",
    "    for i in parkinson:       \n",
    "        new_df = splitByTest(i)\n",
    "        for k in range(new_df.shape[0]):\n",
    "            last_df = np.array_split(i,400)\n",
    "            for j in last_df:\n",
    "                parkinson_data.append(extractFeatures(j))\n",
    "    return pd.DataFrame(parkinson_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the classes\n",
    "\n",
    "Last but not least we will get the classes, this is easy we just make an array of 0s and 1s based on how many control and parkinson clases we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClasses(control, parkinson):\n",
    "    classes = []\n",
    "    for i in range(control.shape[0]):\n",
    "        classes.append(0)\n",
    "    for i in range(parkinson.shape[0]):\n",
    "        classes.append(1)\n",
    "    res = np.array(classes)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We have set the functions to get the features in place, now is time to create our multy layer perceptron. We will set it up so it can recive as many hidden layers as we want. This will also allow you to do you own testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(data, classes, test_data, test_classes,input_size, layer_sizes):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #creating The place holders for our input and output\n",
    "    x = tf.placeholder(tf.float32, [None, input_size])\n",
    "    y = tf.placeholder(tf.float32, [None, 2])\n",
    "    #the lists of weights and biases will start empty but the layers will begin with x\n",
    "    weights = []\n",
    "    biases = []\n",
    "    layers = [x]\n",
    "    \n",
    "    #set a variable to stor the size of the last layer\n",
    "    last_layer = input_size\n",
    "    \n",
    "    #Iterate through the layers so they all feed into eachother\n",
    "    for layer, neurons in enumerate(layer_sizes):  \n",
    "        weights = weights + [tf.Variable(tf.zeros([last_layer, neurons]),name=\"layer\" + str(len(layers)) + \"_weights\")]\n",
    "                       \n",
    "        biases = biases + [tf.Variable(tf.zeros([neurons]),name=\"layer\" + str(len(layers)) + \"_biases\")]\n",
    "                       \n",
    "        layers += [tf.sigmoid(tf.add(tf.matmul(layers[len(layers)-1],weights[len(weights)-1]),biases[len(biases)-1]))]\n",
    "        last_layer = neurons\n",
    "    \n",
    "    #Add the the weights biases and last sigmoid fucntion to get the prediction\n",
    "    weights = weights + [tf.Variable(tf.zeros([last_layer, 2]),name=\"last_layer_weights\")]\n",
    "    biases = biases +[tf.Variable(tf.zeros([2]),name=\"last_layer_biases\")]                                                 \n",
    "    prediction = tf.add(tf.matmul(layers[len(layers)-1],weights[len(weights)-1]),biases[len(biases)-1])\n",
    "    \n",
    "    #Both of this loss functions work properly although the sigmoid one gives slightly better results\n",
    "    #loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=prediction)\n",
    "    \n",
    "    #the predicted class will be the one with the highest value\n",
    "    class_pred = tf.argmax(prediction, axis=1)\n",
    "    \n",
    "    #set upt the optimizer\n",
    "    lr = 0.0005\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    #Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    training_epochs = 1000 *14\n",
    "    train_n_samples = data.shape[0]\n",
    "    display_step = 2000\n",
    "\n",
    "    mini_batch_size = 250\n",
    "    n_batch = train_n_samples // mini_batch_size + (train_n_samples % mini_batch_size != 0)\n",
    "    \n",
    "    #Start training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(training_epochs):\n",
    "            i_batch = (epoch % n_batch)*mini_batch_size\n",
    "            batch = data[i_batch:i_batch+mini_batch_size, :], classes[i_batch:i_batch+mini_batch_size, :]\n",
    "            sess.run(optimizer, feed_dict={x: batch[0], y: batch[1]})\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                cost = sess.run(loss, feed_dict={x: batch[0], y: batch[1]})\n",
    "                acc = np.sum(np.argmax(test_classes, axis=1) == sess.run(class_pred, feed_dict={x: test_data}))/test_classes.shape[0]\n",
    "                print(\"Epoch:\", str(epoch+1), \"Error:\", np.mean(cost), \"Accuracy:\", acc)\n",
    "        parameters = sess.run(weights+biases)\n",
    "        test_predictions = sess.run(class_pred, feed_dict={x: test_data})\n",
    "    return parameters, test_predictions, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data\n",
    "\n",
    "Now let's see how we set up our data in order the feed it to the MLP. First lets load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = getControlFeatures()\n",
    "park = getParkinsonFeatures()\n",
    "target = getClasses(control,park)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to put it together in order to get our train and test split. We will use numpy to concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate((control,park),axis= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get our split in two ways, one is using the StratifiedKFold function, and the other the train_test_split function. I will put the code to both below but we will only use the train_test_split since result were significantly better. It is important to use the random state sin when concatenating the data we are following a very strict order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "skf = StratifiedKFold(n_splits=50,shuffle=True)\n",
    "\n",
    "for train_index, test_index in skf.split(all_data, target):\n",
    "    train_data, test_data = all_data[train_index], all_data[test_index]\n",
    "    train_target, test_target = target[train_index], target[test_index]\n",
    "'''\n",
    "train_data, test_data, train_target, test_target = train_test_split(all_data,target,\n",
    "                                                                    test_size= 0.3, random_state=30 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since our last layer has 2 nodes in it we have to modify our target information to also have 2 columns. The best way of doing this is using the one hot techniche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = np.unique(train_target).shape[0]\n",
    "train_target = np.eye(n_classes)[train_target]\n",
    "test_target = np.eye(n_classes)[test_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final touches before feeding the data to the MLP. We will get the input size by checking how many columns our training data has, and we will also define the amount and the size of the hidden layers, for this particular case, one hidden layer of size seven works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "input_size = np.shape(train_data)[1]\n",
    "print(input_size)\n",
    "layers = np.array([7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the MLP\n",
    "Now we can feed all this to our MLP and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 Error: 0.5426574 Accuracy: 0.7096481682988756\n",
      "Epoch: 4000 Error: 0.4662113 Accuracy: 0.7214726151614074\n",
      "Epoch: 6000 Error: 0.43344012 Accuracy: 0.7977511788175553\n",
      "Epoch: 8000 Error: 0.36063316 Accuracy: 0.8176278563656147\n",
      "Epoch: 10000 Error: 0.3869553 Accuracy: 0.8121508886470802\n",
      "Epoch: 12000 Error: 0.38533595 Accuracy: 0.830177729416032\n",
      "Epoch: 14000 Error: 0.33761773 Accuracy: 0.8355458832063838\n",
      "[[ 5545  2230]\n",
      " [ 2304 17491]]\n"
     ]
    }
   ],
   "source": [
    "params, test_preds, _  = MLP(train_data, train_target, test_data, test_target,input_size, layers)\n",
    "cont1 =np.count_nonzero(test_preds)\n",
    "cont0 = len(test_preds) - cont1\n",
    "print(confusion_matrix(test_preds, np.argmax(test_target, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above with barely any training time we can solve this problem with pretty good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with a varying learning rate\n",
    "\n",
    "One of the biggest problems with the solution proposed above is the fact that all the data is processed as one and thus when making splits of different sizes we are also making testing sets with tempered qualities. In order to tackle this problem I have come up with this solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in the Features\n",
    "\n",
    "The first change is that we will reduce the amount of splits by 10. Although for the training dataset we will continue the 7:4 ratio our testing data will be gotten from a  different set of files and both will be split in the exact same size, 40. wich in order not to favour the control data I have chosen the size of the parkinson data split with the hope to highlight the value of this solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_path = \"DataSplit/control\"\n",
    "parkinson_path = \"DataSplit/parkinson\"\n",
    "control_pathT = \"DataSplit/a\"\n",
    "parkinson_pathT = \"DataSplit/b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine our function to extract the features from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getControlFeatures():\n",
    "    control_data = []\n",
    "    control = getFiles(control_path)\n",
    "    for i in control:\n",
    "        new_df = splitByTest(i)\n",
    "        for k in range(new_df.shape[0]):\n",
    "            last_df = np.array_split(i,70)\n",
    "            for j in last_df:\n",
    "                control_data.append(extractFeatures(j))\n",
    "    return pd.DataFrame(control_data)\n",
    "\n",
    "def getControlFeaturesTest():\n",
    "    control_dataT = []\n",
    "    control = getFiles(control_pathT)\n",
    "    for i in control:\n",
    "        new_df = splitByTest(i)\n",
    "        for k in range(new_df.shape[0]):\n",
    "            last_df = np.array_split(i,40)\n",
    "            for j in last_df:\n",
    "                control_dataT.append(extractFeatures(j))\n",
    "    return pd.DataFrame(control_dataT)\n",
    "\n",
    "def getParkinsonFeatures():\n",
    "    parkinson_data = []\n",
    "    parkinson = getFiles(parkinson_path)\n",
    "    for i in parkinson:\n",
    "        new_df = splitByTest(i)\n",
    "        for k in range(new_df.shape[0]):\n",
    "            last_df = np.array_split(i,40)\n",
    "            for j in last_df:\n",
    "                parkinson_data.append(extractFeatures(j))\n",
    "    return pd.DataFrame(parkinson_data)\n",
    "\n",
    "def getParkinsonFeaturesTest():\n",
    "    parkinson_dataT = []\n",
    "    parkinson = getFiles(parkinson_pathT)\n",
    "    for i in parkinson:\n",
    "        new_df = splitByTest(i)\n",
    "        for k in range(new_df.shape[0]):\n",
    "            last_df = np.array_split(i,40)\n",
    "            for j in last_df:\n",
    "                parkinson_dataT.append(extractFeatures(j))\n",
    "    return pd.DataFrame(parkinson_dataT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to the MLP\n",
    "\n",
    "As you will be able to see the changes are not too crazy will add an if in our training cycle that every X amount of epoch it will reduce the learning rate by half. This modification will allow the optimizers to take bigger steps to escape the local minima of predicting everything as 1 and after it has escaped by reducing the learning rate it will start to learn the details it needs in order to get the control cases too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_MLP(data, classes, test_data, test_classes,input_size, layer_sizes):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, input_size])\n",
    "    y = tf.placeholder(tf.float32, [None, 2])\n",
    "    weights = []\n",
    "    biases = []\n",
    "    layers = [x]\n",
    "    \n",
    "    last_layer = input_size\n",
    "    for layer, neurons in enumerate(layer_sizes):  \n",
    "        weights = weights + [tf.Variable(tf.zeros([last_layer, neurons]),name=\"layer\" + str(len(layers)) + \"_weights\")]\n",
    "                       \n",
    "        biases = biases + [tf.Variable(tf.zeros([neurons]),name=\"layer\" + str(len(layers)) + \"_biases\")]\n",
    "                       \n",
    "        layers += [tf.sigmoid(tf.add(tf.matmul(layers[len(layers)-1],weights[len(weights)-1]),biases[len(biases)-1]))]\n",
    "        last_layer = neurons\n",
    "        \n",
    "    weights = weights + [tf.Variable(tf.zeros([last_layer, 2]),name=\"last_layer_weights\")]\n",
    "    biases = biases +[tf.Variable(tf.zeros([2]),name=\"last_layer_biases\")]                                                 \n",
    "    prediction = tf.add(tf.matmul(layers[len(layers)-1],weights[len(weights)-1]),biases[len(biases)-1])\n",
    "\n",
    "    #loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=prediction)\n",
    "    class_pred = tf.argmax(prediction, axis=1)\n",
    "    \n",
    "    #starting LR\n",
    "    lr = 0.005\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #The number of epochs has also been upped by a bit since this is a more complex problem\n",
    "    training_epochs = 1000 *62\n",
    "    train_n_samples = data.shape[0]\n",
    "    display_step = 2000\n",
    "\n",
    "    mini_batch_size = 100\n",
    "    n_batch = train_n_samples // mini_batch_size + (train_n_samples % mini_batch_size != 0)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(training_epochs):\n",
    "            i_batch = (epoch % n_batch)*mini_batch_size\n",
    "            batch = data[i_batch:i_batch+mini_batch_size, :], classes[i_batch:i_batch+mini_batch_size, :]\n",
    "            sess.run(optimizer, feed_dict={x: batch[0], y: batch[1]})\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                cost = sess.run(loss, feed_dict={x: batch[0], y: batch[1]})\n",
    "                acc = np.sum(np.argmax(test_classes, axis=1) == sess.run(class_pred, feed_dict={x: test_data}))/test_classes.shape[0]\n",
    "                print(\"Epoch:\", str(epoch+1), \"Error:\", np.mean(cost), \"Accuracy:\", acc)\n",
    "            #This if will reduces the learning rate in half every 150k epochs\n",
    "            if epoch % 30000 == 0:\n",
    "                lr = lr *0.5\n",
    "                optimizer.learning_rate = lr\n",
    "        parameters = sess.run(weights+biases)\n",
    "        test_predictions = sess.run(class_pred, feed_dict={x: test_data})\n",
    "    return parameters, test_predictions, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the data\n",
    "Since we have processed the training and the testing data in a better way we no longer need the train test split function and we can load them directly, The files that where chosen and testing data where chosen as random as to not to tamper with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getControlFeatures()\n",
    "park = getParkinsonFeatures()\n",
    "\n",
    "test_control = getControlFeaturesTest()\n",
    "test_park = getParkinsonFeaturesTest()\n",
    "\n",
    "all_data = np.concatenate((data,park),axis= 0)\n",
    "all_dataTest = np.concatenate((test_control,test_park),axis= 0)\n",
    "\n",
    "target = getClasses(data,park)\n",
    "targetTest = getClasses(test_control,test_park)\n",
    "\n",
    "#One hot encoding\n",
    "n_classes = np.unique(target).shape[0]\n",
    "train_target = np.eye(n_classes)[target]\n",
    "test_target = np.eye(n_classes)[targetTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding the data into the new MLP\n",
    "\n",
    "Now we set up the last deatils and test how our new MLP performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Epoch: 2000 Error: 0.15407906 Accuracy: 0.7727272727272727\n",
      "Epoch: 4000 Error: 0.29726428 Accuracy: 0.7727272727272727\n",
      "Epoch: 6000 Error: 0.47202072 Accuracy: 0.7659090909090909\n",
      "Epoch: 8000 Error: 1.0311526 Accuracy: 0.790340909090909\n",
      "Epoch: 10000 Error: 0.16155408 Accuracy: 0.7659090909090909\n",
      "Epoch: 12000 Error: 0.19617914 Accuracy: 0.7670454545454546\n",
      "Epoch: 14000 Error: 0.37720585 Accuracy: 0.7693181818181818\n",
      "Epoch: 16000 Error: 0.9635094 Accuracy: 0.7886363636363637\n",
      "Epoch: 18000 Error: 0.16256982 Accuracy: 0.7852272727272728\n",
      "Epoch: 20000 Error: 0.15093979 Accuracy: 0.7670454545454546\n",
      "Epoch: 22000 Error: 0.30798447 Accuracy: 0.7465909090909091\n",
      "Epoch: 24000 Error: 0.38509345 Accuracy: 0.7653409090909091\n",
      "Epoch: 26000 Error: 1.0645844 Accuracy: 0.772159090909091\n",
      "Epoch: 28000 Error: 0.1516212 Accuracy: 0.7551136363636364\n",
      "Epoch: 30000 Error: 0.16407265 Accuracy: 0.7926136363636364\n",
      "Epoch: 32000 Error: 0.37569228 Accuracy: 0.8022727272727272\n",
      "Epoch: 34000 Error: 0.9489357 Accuracy: 0.7954545454545454\n",
      "Epoch: 36000 Error: 0.13188891 Accuracy: 0.8\n",
      "Epoch: 38000 Error: 0.11572575 Accuracy: 0.7664772727272727\n",
      "Epoch: 40000 Error: 0.24017906 Accuracy: 0.7767045454545455\n",
      "Epoch: 42000 Error: 0.40105402 Accuracy: 0.7636363636363637\n",
      "Epoch: 44000 Error: 0.9465075 Accuracy: 0.7664772727272727\n",
      "Epoch: 46000 Error: 0.11222729 Accuracy: 0.7579545454545454\n",
      "Epoch: 48000 Error: 0.16877455 Accuracy: 0.7977272727272727\n",
      "Epoch: 50000 Error: 0.4354223 Accuracy: 0.7551136363636364\n",
      "Epoch: 52000 Error: 0.9138778 Accuracy: 0.7767045454545455\n",
      "Epoch: 54000 Error: 0.11675991 Accuracy: 0.790340909090909\n",
      "Epoch: 56000 Error: 0.12557578 Accuracy: 0.7960227272727273\n",
      "Epoch: 58000 Error: 0.22094582 Accuracy: 0.7801136363636364\n",
      "Epoch: 60000 Error: 0.39810342 Accuracy: 0.8011363636363636\n",
      "Epoch: 62000 Error: 0.94322133 Accuracy: 0.8119318181818181\n",
      "[[ 153   84]\n",
      " [ 247 1276]]\n"
     ]
    }
   ],
   "source": [
    "input_size = np.shape(all_data)[1]\n",
    "print(input_size)\n",
    "layers = np.array([7])\n",
    "params, test_preds, _  = custom_MLP(all_data, train_target, all_dataTest, test_target,input_size, layers)\n",
    "\n",
    "print(confusion_matrix(test_preds, np.argmax(test_target, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe even if the accuracy is not as good, it is barely 3% worse and we have managed to get a better regularization by not making different splits in the testing date for each class. I would consider these a far better solution. If we also look at the confusion matrix we see that it has learned perfectly well the difference from both classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
